1、表结构设计
根据视频云项目需求，分六张表存储数据，
分别存储人(HumanTable)，车(VehicleTable)，未知物体(UnkonwnTable)的元数据和特征数据，
以及人、车、未知物表对应的图片表,存储图片的路径，其中特征表和图片表具有相同的rowkey。

根据hbase分布式数据库的特点，family越多，那么读取每一个cell数据的优势越明显，因为网络和IO都减少了，
而如果只有一个family，那么每一次都会读取当前rowkey的所有数据，网络和IO上会有一些损失，

但是column family太多在flush的时候，它邻近的column family也会因关联效应被触发flush，最终导致系统产生更多的I/O，

根据视频云需求，将固定返回的几列数据放在一个列族里面，只需一次请求即可拿回该列族上的数据，
因此，在设计表结构的时候，将元数据，特征数据分为两个列族来存储，
如果后期需要加上其他特征数据，只需在该列族上增加对应的列即可，保证良好的可扩展性。

2、行键的设计
Rowkey：HashValue #SrcType#CameraId#FileId#Time(yymmddhhmmss) 
Rowkey解释：
HashValue：每个region分配一个唯一的hashvalue，用于决定将记录写入哪一个region，固定4位整数
SrcType：目标类型
CameraId：摄像头ID，固定位数整数
FileId：文件ID
Time（yymmddhhmmss）：年月日时分秒
Rowkey字段拼接顺序需要根据查询优先级调整，提高查询效率。

3、Flush&Compaction策略
a)	根据需求，hbase表单条记录平均大约在100K左右，如果使用常规方式，会对集群造成较大压力
b)	尽量减少compaction和flush对持续存储造成的冲击，将flush size调整到512M，保证每次flush的hfile超过512M，
同时禁止掉minor compaction和自动的major compaction，这样保证即使HTABLE膨胀到TB级别，集群写入性能也不会受到影响
c)	通过设置bloomfilter保证集群读取性能，即使每个region包含上百个hfile，查询性能也不会受到太大影响，
默认情况下创建table是不打开bloomfilter的（可通过describe table来确认，如看到BLOOMFILTER => ‘NONE’则表示未打开）
对于随机读而言这个影响还是比较明显的，由于bloomfilter无法在之后动态打开，因此创建表时最好就处理好，
方法类似如此：create ‘t1′, { NAME => ‘f1′, BLOOMFILTER => ‘ROWCOL’ }。

4、Region划分
Region的划分根据机器的配置而定，原则上一个线程处理一个region的数据。
a)	考虑到并发查询的需求，htable不宜划分太多的region，每张表在每个regionserver上保持24（一个刀片处理的线程数）个region。
b)	以4个节点集群为例，一共划分24（一个刀片处理的线程数）*4=96个region，
预定义每个region前缀：0001-0096， 通过hash函数预切分region，
每条记录需要生成唯一的hash前缀（0001-0096），通过hash前缀判断应该放在哪个region中，
这样保证写入请求均匀分布在每台regionserver上。
c)	Region预切分好后不会再改变，只有region大小会不断膨胀，达到一个月之后，重新建立新的表存储新数据。

5、缓存策略
a)	为了提高查询速度，可以将最近写入数据进行缓存，开启HBase的in-memory htable和cacheOnWrite功能，
将最近写入数据同时缓存到内存中。

