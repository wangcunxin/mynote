1.黑马论坛日志是apache服务器产生的common日志，用户的每一次对网站的访问请求会产生一条记录。
2.每天大约产生200MB，我们需要做的是对服务器上的数据进行统计分析。
3.统计指标
3.1 PV，指的是页面浏览量。
    计算方法：对记录统计行数。
3.2 注册用户数，
    计算方法：计算请求路径中member.php?mod=register的出现次数。
3.3 独立ip，访问网站的不同的ip
    计算方法：对记录中出现的ip，去重，计数
3.4 跳出数，指的是某ip只访问了一次，就不再出现了。
    计算方法：对ip计数，统计出现次数是1次的ip
3.5 版块热度
    计算方法：对版块进行计数汇总
 
4.操作步骤
4.1 使用flume把日志数据从普通的文件夹加载到HDFS中
4.2 使用mapreduce数据清洗,见java代码
    (1)这是执行脚本，其中时间是动态的
	   hadoop jar cleaner.jar  hdfs://hadoop0:9000/hmbbs/$(date +%Y-%m-%d)  hdfs://hadoop0:9000/hmbbs_cleaned/$(date +%Y-%m-%d)
    (2)配置到crontab中，每天1点自动执行，把flume导入的数据清洗后送到/hmbbs_cleaned目录中
	   * 1 * * *   hadoop jar cleaner.jar  hdfs://hadoop0:9000/hmbbs/$(date +%Y-%m-%d)  hdfs://hadoop0:9000/hmbbs_cleaned/$(date +%Y-%m-%d)
	   
4.3 使用hive对hdfs中的日志文件进行统计分析，获得统计指标
        (1)对清洗后的数据文件，创建外部分区表
	   CREATE EXTERNAL TABLE hmbbs_cleaned(ip string, logtime string, url string) PARTITIONED BY(ltime string) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';
	(2)把右边代码放到crontab中，自动运行hive -e "ALTER TABLE hmbbs_cleaned ADD PARTITION(ltime='$(date +%Y-%m-%d)') LOCATION '/hmbbs_cleaned/$(date +%Y-%m-%d)'";
	ALTER TABLE hmbbs_cleaned ADD PARTITION(ltime='2013-12-09') LOCATION '/hmbbs_cleaned/2013-12-09';
	 ALTER TABLE hmbbs_cleaned ADD PARTITION(ltime='2013-05-31') LOCATION '/hmbbs_cleaned/2013-05-31';
	(3)统计PV插入到pv_tmp表
	CREATE TABLE pv_tmp(ltime string, pv int);
	INSERT INTO TABLE pv_tmp SELECT '$(date +%Y-%m-%d)', COUNT(1) FROM hmbbs_cleaned WHERE ltime='$(date +%Y-%m-%d)';
	INSERT INTO TABLE pv_tmp SELECT '2013-12-09', COUNT(1) FROM hmbbs_cleaned WHERE ltime='2013-12-09';
	(4)注册用户数
	CREATE TABLE reg_tmp(ltime string, reg int);
	INSERT INTO TABLE reg_tmp SELECT '2013-12-04', COUNT(1) FROM hmbbs_cleaned WHERE ltime='2013-12-04' AND sub(url,'member.php?mod=register')=1;
	(5)独立ip
	CREATE TABLE ip_tmp(ltime string, ip int);
	INSERT INTO TABLE ip_tmp SELECT '2013-12-04', COUNT(distinct ip) FROM hmbbs_cleaned WHERE ltime='2013-12-04';
	(6)跳出数
	CREATE TABLE jump_tmp(ltime string, jump int);
	 INSERT INTO TABLE jump_tmp SELECT '2013-12-04', COUNT(1) FROM (SELECT COUNT(ip) cip FROM hmbbs_cleaned WHERE ltime='2013-12-04' GROUP BY ip HAVING cip=1) t;
	(7)把以上表数据统一到一个内部分区表hmbbs_all
	CREATE TABLE hmbbs_all(ltime string, pv int, reg int, ip int, jump int) PARTITIONED BY (lt string);
	INSERT INTO TABLE hmbbs_all  PARTITION(lt='2013-12-04') SELECT pv_tmp.ltime, pv_tmp.pv, reg_tmp.reg, ip_tmp.ip, jump_tmp.jump FROM pv_tmp JOIN reg_tmp ON (pv_tmp.ltime=reg_tmp.ltime) JOIN ip_tmp ON  (pv_tmp.ltime=ip_tmp.ltime) JOIN jump_tmp ON  (pv_tmp.ltime=jump_tmp.ltime) WHERE pv_tmp.ltime='2013-12-04';
4.4 使用sqoop把hive统计结果导出到mysql中
    sqoop-export --connect jdbc:mysql://192.168.80.100:3306/hmbbs --username root --password admin --table hmbbs --export-dir '/hive/hmbbs_all' --fields-terminated-by '\001';
4.5 使用linux的crontab让hive操作、sqoop操作周期性执行
4.6 使用hbase对ip、date进行详细查询

---------------------------------------------------------------------------
clean.sh

#!/bin/bash
hadoop jar cleaner.jar  hdfs://hadoop0:9000/hmbbs/$(date +%Y-%m-%d)  hdfs://hadoop0:9000/hmbbs_cleaned/$(date +%Y-%m-%d)

insert.sh

#!/bin/bash
cd hive/bin   
hive
use hive
ALTER TABLE hmbbs_cleaned ADD PARTITION(ltime='$(date +%Y-%m-%d)') LOCATION '/hmbbs_cleaned/$(date +%Y-%m-%d)';
#pv
INSERT INTO TABLE pv_tmp SELECT '$(date +%Y-%m-%d)', COUNT(1) FROM hmbbs_cleaned WHERE ltime='$(date +%Y-%m-%d)';
#注册用户数uv
add jar SubContain.jar;
create temporary function sub as ‘cn.itcast.hmbbs.SubContain’;

INSERT INTO TABLE reg_tmp SELECT '$(date +%Y-%m-%d)', COUNT(1) FROM hmbbs_cleaned WHERE ltime='$(date +%Y-%m-%d)' AND sub(url,'member.php?mod=register')=1;
#独立ip
INSERT INTO TABLE ip_tmp SELECT '$(date +%Y-%m-%d)', COUNT(distinct ip) FROM hmbbs_cleaned WHERE ltime='$(date +%Y-%m-%d)';
#跳出数
INSERT INTO TABLE jump_tmp SELECT '$(date +%Y-%m-%d)', COUNT(1) FROM (SELECT COUNT(ip) cip FROM hmbbs_cleaned WHERE ltime='$(date +%Y-%m-%d)' GROUP BY ip HAVING cip=1) t;
#把以上表数据统一到一个内部分区表hmbbs_all
INSERT INTO TABLE hmbbs_all  PARTITION(lt='$(date +%Y-%m-%d)') SELECT pv_tmp.ltime, pv_tmp.pv, reg_tmp.reg, ip_tmp.ip, jump_tmp.jump FROM pv_tmp JOIN reg_tmp ON (pv_tmp.ltime=reg_tmp.ltime) JOIN ip_tmp ON  (pv_tmp.ltime=ip_tmp.ltime) JOIN jump_tmp ON  (pv_tmp.ltime=jump_tmp.ltime) WHERE pv_tmp.ltime='$(date +%Y-%m-%d)';
quit;


export.sh
#!/bin/bash
sqoop-export --connect jdbc:mysql://192.168.80.100:3306/hmbbs --username root --password admin --table hmbbs --export-dir '/hive/hmbbs_all' --fields-terminated-by '\001';

#创建定时任务：crontab -e
* 1 * * *  -u root  clean.sh,insert.sh,export.sh


-----------------------------------------------
查看当前用户的定时crontab 任务:
cat /var/spool/cron/root

crontab基本格式 :
 *　　*　　*　　*　　*　　command
 分　时　日　月　周　命令 

第1列表示分钟1～59 每分钟用*或者 */1表示
 第2列表示小时1～23（0表示0点）
 第3列表示日期1～31
 第4列表示月份1～12
 第5列标识号星期0～6（0表示星期天）
 第6列要运行的命令 


hive自定义函数
#在hive中添加包：
hive>add jar SubContain.jar;
#创建临时函数
hive>create temporary function sub as ‘cn.itcast.hmbbs.SubContain’;

基础函数需要直接集成到hive：
方式一：编写/root/.hiverc 把命令写入，每次启动hive时，这个文件就会调用；
方式二：
1.添加函数文件$HIVE_HOME/src/ql/src/java/org/apache/hadoop/hive/ql/udf/目录下；
2.将函数sub注册到hive的函数列表中
 修改$HIVE_HOME/src/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java文件
 import org.apache.hadoop.hive.ql.udf.SubContain;
 将registerUDF(“sub”, SubContain.class,false);写在static代码块中。

